{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deteksi Emosi Pengguna Twitter\n",
        "\n",
        "Deteksi emosi merupakan salah satu permasalahan yang dihadapi pada ***Natural Language Processing*** (NLP). Alasanya diantaranya adalah kurangnya dataset berlabel untuk mengklasifikasikan emosi berdasarkan data twitter. Selain itu, sifat dari data twitter yang dapat memiliki banyak label emosi (***multi-class***). Manusia memiliki berbagai emosi dan sulit untuk mengumpulkan data yang cukup untuk setiap emosi. Oleh karena itu, masalah ketidakseimbangan kelas akan muncul (***class imbalance***). Pada Ujian Tengah Semester (UTS) kali ini, Anda telah disediakan dataset teks twitter yang sudah memiliki label untuk beberapa kelas emosi. Tugas utama Anda adalah membuat model yang mumpuni untuk kebutuhan klasifikasi emosi berdasarkan teks.\n",
        "\n",
        "### Informasi Data\n",
        "\n",
        "Dataset yang akan digunakan adalah ****tweet_emotion.csv***. Berikut merupakan informasi tentang dataset yang dapat membantu Anda.\n",
        "\n",
        "- Total data: 40000 data\n",
        "- Label emosi: anger, boredom, empty, enthusiasm, fun, happiness, hate, love, neutral, relief, sadness, surprise, worry\n",
        "- Jumlah data untuk setiap label tidak sama (***class imbalance***)\n",
        "- Terdapat 3 kolom = 'tweet_id', 'sentiment', 'content'\n",
        "\n",
        "### Penilaian UTS\n",
        "\n",
        "UTS akan dinilai berdasaarkan 4 proses yang akan Anda lakukan, yaitu pra pengolahan data, ektraksi fitur, pembuatan model machine learning, dan evaluasi.\n",
        "\n",
        "#### Pra Pengolahan Data\n",
        "\n",
        "> **Perhatian**\n",
        "> \n",
        "> Sebelum Anda melakukan sesuatu terhadap data Anda, pastikan data yang Anda miliki sudah \"baik\", bebas dari data yang hilang, menggunakan tipe data yang sesuai, dan sebagainya.\n",
        ">\n",
        "\n",
        "Data tweeter yang ada dapatkan merupakan sebuah data mentah, maka beberapa hal dapat Anda lakukan (namun tidak terbatas pada) yaitu,\n",
        "\n",
        "1. Case Folding\n",
        "2. Tokenizing\n",
        "3. Filtering\n",
        "4. Stemming\n",
        "\n",
        "*CATATAN: PADA DATA TWITTER TERDAPAT *MENTION* (@something) YANG ANDA HARUS TANGANI SEBELUM MASUK KE TAHAP EKSTRAKSI FITUR*\n",
        "\n",
        "#### Ekstrasi Fitur\n",
        "\n",
        "Anda dapat menggunakan beberapa metode, diantaranya\n",
        "\n",
        "1. Bag of Words (Count / TF-IDF)\n",
        "2. N-gram\n",
        "3. dan sebagainya\n",
        "\n",
        "#### Pembuatan Model\n",
        "\n",
        "Anda dibebaskan dalam memilih algoritma klasifikasi. Anda dapat menggunakan algoritma yang telah diajarkan didalam kelas atau yang lain, namun dengan catatan. Berdasarkan asas akuntabilitas pada pengembangan model machine learning, Anda harus dapat menjelaskan bagaimana model Anda dapat menghasilkan nilai tertentu.\n",
        "\n",
        "#### Evaluasi\n",
        "\n",
        "Pada proses evaluasi, minimal Anda harus menggunakan metric akurasi. Akan tetapi Anda juga dapat menambahkan metric lain seperti Recall, Precision, F1-Score, detail Confussion Metric, ataupun Area Under Curve (AUC)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n",
            "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n",
            "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n",
            "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # Linear Algebra\n",
        "import pandas as pd # Data processing\n",
        "from wordcloud import WordCloud # Visualization\n",
        "import regex as re # Text processing\n",
        "import matplotlib.pyplot as plt # Visualization\n",
        "from textblob import Word, TextBlob # Text features\n",
        "import nltk # Text Manipulation\n",
        "nltk.download('wordnet') # Download wordnet\n",
        "nltk.download('punkt') # Download punkt\n",
        "nltk.download(\"stopwords\") # Download Stopwords\n",
        "nltk.download(\"omw-1.4\")  # Download omw-1.4\n",
        "from nltk.corpus import words as dict_words #contains english word dictionary\n",
        "from nltk.corpus import stopwords #contains stop words\n",
        "from sklearn.model_selection import train_test_split # split out data into training and testing sets\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P1_ujOvPL74O"
      },
      "outputs": [],
      "source": [
        "original_data_frame = pd.read_csv(\"data/tweet_emotions.csv\")\n",
        "# Membuat salinan dataframe untuk manipulasi\n",
        "dataset = original_data_frame.copy()\n",
        "dataset = dataset[['content','sentiment']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "urdjmJWBL_qw",
        "outputId": "8f8f0d21-7eda-400d-beb9-459b20f464ac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "      <td>empty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "      <td>enthusiasm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>@sweeetnspicy hiii im on my ipod...i cant fall...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>dont wanna work 11-830 tomorrow  but i get paid</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>feels sad coz i wasnt able to play with the gu...</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>PrinceCharming</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>@ cayogial i wanted to come to BZ this summer ...</td>\n",
              "      <td>hate</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              content   sentiment\n",
              "0   @tiffanylue i know  i was listenin to bad habi...       empty\n",
              "1   Layin n bed with a headache  ughhhh...waitin o...     sadness\n",
              "2                 Funeral ceremony...gloomy friday...     sadness\n",
              "3                wants to hang out with friends SOON!  enthusiasm\n",
              "4   @dannycastillo We want to trade with someone w...     neutral\n",
              "..                                                ...         ...\n",
              "95  @sweeetnspicy hiii im on my ipod...i cant fall...     sadness\n",
              "96    dont wanna work 11-830 tomorrow  but i get paid     sadness\n",
              "97  feels sad coz i wasnt able to play with the gu...     sadness\n",
              "98                                     PrinceCharming     neutral\n",
              "99  @ cayogial i wanted to come to BZ this summer ...        hate\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.head(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CLEANING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mCPSCMeJMATk"
      },
      "outputs": [],
      "source": [
        "def remove_unwanted_text(content):\n",
        "  '''\n",
        "  Removes unwanted text from content using regex\n",
        "  Input\n",
        "  content: A string\n",
        "  Output\n",
        "  final: the final parsed string\n",
        "  '''\n",
        "  handle = re.sub('@[^\\s]+', '', content)\n",
        "  link = re.sub('http[^\\s]+', '', handle)\n",
        "  link = re.sub('www[^\\s]+', '', link)\n",
        "  ht = re.sub('#[^\\s]+', '', link)\n",
        "  final = re.sub('&[^\\s]+', '', ht)\n",
        "  pa = re.sub('^[^\\s]+', '', final)\n",
        "\n",
        "  return pa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-ZbQ8F7FMiJA"
      },
      "outputs": [],
      "source": [
        "def stem(words):\n",
        "  '''\n",
        "  Input\n",
        "  words: words to be processed\n",
        "  Output\n",
        "  returns array of stemmed words\n",
        "  '''\n",
        "  tb = TextBlob(' '.join(words))\n",
        "  return [w for w in tb.words.stem()]\n",
        "\n",
        "def lemmatize(words):\n",
        "  '''\n",
        "  Input\n",
        "  words: words to be processed\n",
        "  Output\n",
        "  returns array of lemmatized words\n",
        "  '''\n",
        "  tb = TextBlob(' '.join(words))\n",
        "  return [w for w in tb.words.lemmatize()]\n",
        "\n",
        "def correct_words(words):\n",
        "  '''\n",
        "  Corrects a word using TextBlob\n",
        "  Input\n",
        "  words: a list of words\n",
        "  Output\n",
        "  returns corrected words\n",
        "  '''\n",
        "  return [Word(w).correct() for w in words]\n",
        "    \n",
        "def word_frequency(text):\n",
        "  '''\n",
        "  Counts the words in a dataframe\n",
        "  Input\n",
        "  text: text to be counted\n",
        "  Output\n",
        "  word frequecy in text\n",
        "  '''\n",
        " # Tokenization\n",
        "  tb = TextBlob(text)\n",
        "  return tb.word_counts\n",
        "\n",
        "def remove_punctuations(words):\n",
        "  '''\n",
        "  Input\n",
        "  words: A list of words to be processed\n",
        "  Output\n",
        "  returns a list of words that punctuations and numbers have been removed. \n",
        "  '''\n",
        "  new_words = []\n",
        "  for w in words:\n",
        "      l = re.sub('[^A-Za-z ]+', '', w)\n",
        "      if l != '':\n",
        "          new_words.append(l)\n",
        "          \n",
        "  return new_words\n",
        "\n",
        "def remove_stop_words(words):\n",
        "  '''\n",
        "  Input\n",
        "  words: Words to be processed\n",
        "  Output\n",
        "  returns a list of words without english stopwords\n",
        "  '''\n",
        "  sw = stopwords.words(\"english\") # English Stop Words\n",
        "  sw.append('could') # should be in sw since wouldn't and couldn't are in (lemmatization or stem don't convert)\n",
        "  sw.append('would') # should be in sw since wouldn't and couldn't are in (lemmatization or stem don't convert)\n",
        "  sw.append('nt') # nt appears a lot\n",
        "  sw.append('im') # im appears a lot\n",
        "  sw = remove_punctuations(sw)\n",
        "\n",
        "  return [w for w in words if w.lower() not in sw]\n",
        "\n",
        "def clean_data(content):\n",
        "  '''\n",
        "  Cleans the incoming data\n",
        "  Input\n",
        "  content: a dataframe series\n",
        "  '''\n",
        "  for i in range(len(content)):\n",
        "      tweet = remove_unwanted_text(content[i].lower())\n",
        "      tb = TextBlob(tweet)\n",
        "      words = remove_punctuations(tb.words)\n",
        "      words = remove_stop_words(words)\n",
        "\n",
        "      lemmatized_words = lemmatize(words) # for the most part this was good\n",
        "      words = [w for w in lemmatized_words if len(w)>1] # removes any character of len 1 usually a \"u\" or \"n\"\n",
        "      dataset.loc[i, 'content'] = \" \".join(words)\n",
        "\n",
        "def get_array(text):\n",
        "  '''\n",
        "  Returns the text into an array format\n",
        "  '''\n",
        "  return text.split(\" \")      \n",
        "\n",
        "def get_entire_text(content):\n",
        "  '''\n",
        "  Return the entire text in a dataframe series\n",
        "  '''\n",
        "  entire_text = \"\"\n",
        "  for i in content:\n",
        "      entire_text += i + \" \"\n",
        "  return entire_text\n",
        "\n",
        "def display_top_words(wf, k=20):\n",
        "    '''\n",
        "    Displays the top words in a data set\n",
        "    Input\n",
        "    k: Number of top words to display. Defaults to 20.\n",
        "    Output displays top [k] values\n",
        "    '''\n",
        "    wf_sorted = sorted(wf.items(), key=lambda x: x[1], reverse=True)\n",
        "    wf_sorted_greater_than_1 = [w for w in wf_sorted if w[1] > 1]\n",
        "    top = wf_sorted_greater_than_1[:k:]\n",
        "    x = [x[0] for x in top]\n",
        "    y = [y[1] for y in top]\n",
        "    %matplotlib inline\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.bar(x, y)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "def display_wordcloud(words):\n",
        "    '''\n",
        "    Displays wordcloud\n",
        "    '''\n",
        "    wc = WordCloud()\n",
        "    wc.generate(words)\n",
        "    %matplotlib inline \n",
        "    plt.imshow(wc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2ZOEv_ZbMxu4"
      },
      "outputs": [],
      "source": [
        "clean_data(dataset['content']) # cleans the data\n",
        "text = get_entire_text(dataset['content'])\n",
        "wf = word_frequency(text) # Word Frequency "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "227jo6ciQL4M",
        "outputId": "8573f6d1-0e2d-4bc9-a80c-d53176b5cd98"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>know listenin bad habit earlier started freaki...</td>\n",
              "      <td>empty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bed headache ughhhh waitin call</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ceremony gloomy friday</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hang friend soon</td>\n",
              "      <td>enthusiasm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>want trade someone houston ticket one</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td></td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>mother day love</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>mother day mommy woman man long momma someone day</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>wassup beautiful follow peep new hit single de...</td>\n",
              "      <td>happiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>bullet train tokyo gf visiting japan since thu...</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content   sentiment\n",
              "0      know listenin bad habit earlier started freaki...       empty\n",
              "1                        bed headache ughhhh waitin call     sadness\n",
              "2                                 ceremony gloomy friday     sadness\n",
              "3                                       hang friend soon  enthusiasm\n",
              "4                  want trade someone houston ticket one     neutral\n",
              "...                                                  ...         ...\n",
              "39995                                                        neutral\n",
              "39996                                    mother day love        love\n",
              "39997  mother day mommy woman man long momma someone day        love\n",
              "39998  wassup beautiful follow peep new hit single de...   happiness\n",
              "39999  bullet train tokyo gf visiting japan since thu...        love\n",
              "\n",
              "[40000 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PROCESSING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1QFPFQ-HA6ZN"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "X = dataset['content'].values\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y = lbl_enc.fit_transform(dataset.sentiment.values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset['label'] = y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>know listenin bad habit earlier started freaki...</td>\n",
              "      <td>empty</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bed headache ughhhh waitin call</td>\n",
              "      <td>sadness</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ceremony gloomy friday</td>\n",
              "      <td>sadness</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hang friend soon</td>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>want trade someone houston ticket one</td>\n",
              "      <td>neutral</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>go prom bc bf like friend</td>\n",
              "      <td>worry</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>sleep thinking old friend want married damn wa...</td>\n",
              "      <td>sadness</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td></td>\n",
              "      <td>worry</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>charlene love miss</td>\n",
              "      <td>sadness</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>sorry least friday</td>\n",
              "      <td>sadness</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content   sentiment  label\n",
              "0  know listenin bad habit earlier started freaki...       empty      2\n",
              "1                    bed headache ughhhh waitin call     sadness     10\n",
              "2                             ceremony gloomy friday     sadness     10\n",
              "3                                   hang friend soon  enthusiasm      3\n",
              "4              want trade someone houston ticket one     neutral      8\n",
              "5                          go prom bc bf like friend       worry     12\n",
              "6  sleep thinking old friend want married damn wa...     sadness     10\n",
              "7                                                          worry     12\n",
              "8                                 charlene love miss     sadness     10\n",
              "9                                 sorry least friday     sadness     10"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x1_train,x1_test,y1_train,y1_test = train_test_split(X,y,test_size=1/3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13334,)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(26666,)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x1_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(26666,)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y1_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13334,)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y1_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extracting with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Extracting TF-IDF parameters\n",
        "tfidf = TfidfVectorizer(max_features=1000, analyzer='word', ngram_range=(1,3))\n",
        "x_train_tfidf = tfidf.fit_transform(x1_train)\n",
        "x_val_tfidf = tfidf.fit_transform(x1_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extracting with CountVector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#Extracting Count Vectors Parameters\n",
        "count_vect = CountVectorizer(analyzer='word')\n",
        "count_vect.fit(dataset['content'])\n",
        "x_train_count = count_vect.transform(x1_train)\n",
        "x_val_count = count_vect.transform(x1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL MULTINOMIAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "naive bayes tfidf accuracy 0.20743962801859908\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(x_train_tfidf, y1_train)\n",
        "y_pred = nb.predict(x_val_tfidf)\n",
        "print('naive bayes tfidf accuracy %s' % accuracy_score(y_pred, y1_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LINEAR SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.1735413229338533\n",
            "[[   0    0    1    1    2    2    0    2    4    0    0    0    1]\n",
            " [   0    0    4    0    0    9    2    2    7    0    6    0   10]\n",
            " [   1    1    3    4   12   36    6   31   51    6   42   14   50]\n",
            " [   1    1    4    7   15   26    2   25   27    6   22    8   39]\n",
            " [   2    0    4    8   19   41   10   42   57   23   43   19   64]\n",
            " [   4   16   23   24   51  171   46   91  271   50  152   84  284]\n",
            " [   1    2    6    7   13   54   11   20   80   23   44   22  108]\n",
            " [   2    4   15   21   66  164   37  106  268   65  199   68  277]\n",
            " [  14   11   97   66  137  410  115  278 1018  121  408  203  710]\n",
            " [   0    2    9    4   13   67   10   34   67   12   31   13   59]\n",
            " [   6   15   34   37   88  233   70  179  360   55  271   90  446]\n",
            " [   0    1   11   12   23   69   15   43   93   14   64   36  125]\n",
            " [   6    9   48   54  134  447   88  453  595  116  480  170  660]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        13\n",
            "           1       0.00      0.00      0.00        40\n",
            "           2       0.01      0.01      0.01       257\n",
            "           3       0.03      0.04      0.03       183\n",
            "           4       0.03      0.06      0.04       332\n",
            "           5       0.10      0.13      0.11      1267\n",
            "           6       0.03      0.03      0.03       391\n",
            "           7       0.08      0.08      0.08      1292\n",
            "           8       0.35      0.28      0.31      3588\n",
            "           9       0.02      0.04      0.03       321\n",
            "          10       0.15      0.14      0.15      1884\n",
            "          11       0.05      0.07      0.06       506\n",
            "          12       0.23      0.20      0.22      3260\n",
            "\n",
            "    accuracy                           0.17     13334\n",
            "   macro avg       0.08      0.08      0.08     13334\n",
            "weighted avg       0.20      0.17      0.18     13334\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model 2: Linear SVM\n",
        "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
        "lsvm.fit(x_train_tfidf, y1_train)\n",
        "y_pred = lsvm.predict(x_val_tfidf)\n",
        "print(accuracy_score(y_pred, y1_test))\n",
        "print(confusion_matrix(y_pred, y1_test))\n",
        "\n",
        "print(classification_report(y_pred, y1_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2143392830358482\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    1    2    1    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    1    1    0    0    2    3    1    3    1    2]\n",
            " [   7    7   24   28   56  217   40  115  243   68  168   63  309]\n",
            " [   0    0    1    0    3    5    3    3   17    4    6    2   19]\n",
            " [   3    0    2    8   23   43   12   32   78   17   54   21   63]\n",
            " [  17   24  129  102  240  637  182  412 1440  165  702  303 1161]\n",
            " [   0    0    0    1    1    3    0    3    3    1    2    0    4]\n",
            " [   3    9   25   31   61  195   44  187  269   57  209   85  320]\n",
            " [   0    0    0    0    3   21    2    4   14    3   11   16   16]\n",
            " [   7   22   78   74  185  608  129  547  829  174  607  236  939]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.00      0.00      0.00         0\n",
            "           4       0.00      0.07      0.00        14\n",
            "           5       0.13      0.16      0.14      1345\n",
            "           6       0.01      0.05      0.01        63\n",
            "           7       0.02      0.09      0.04       356\n",
            "           8       0.50      0.26      0.34      5514\n",
            "           9       0.00      0.06      0.00        18\n",
            "          10       0.12      0.14      0.13      1495\n",
            "          11       0.02      0.18      0.04        90\n",
            "          12       0.33      0.21      0.26      4435\n",
            "\n",
            "    accuracy                           0.21     13334\n",
            "   macro avg       0.09      0.09      0.07     13334\n",
            "weighted avg       0.34      0.21      0.26     13334\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Model 3: Logistic Regression\n",
        "logreg = LogisticRegression(C=1)\n",
        "logreg.fit(x_train_tfidf, y1_train)\n",
        "y_pred = logreg.predict(x_val_tfidf)\n",
        "print(accuracy_score(y_pred, y1_test))\n",
        "print(confusion_matrix(y_pred, y1_test))\n",
        "print(classification_report(y_pred, y1_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "COUNT VECTOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.30785960701964904\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0    0    0    1    1    0    1    5    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    1    0    0    0    0    0]\n",
            " [   0    0    2    0    1    5    0    2   10    0    1    0    4]\n",
            " [   6    3   27   41  134  550   13  291  298   92   84  104  162]\n",
            " [   0    0    2    0    0    0   11    1    3    0    0    0    2]\n",
            " [   1    1    4   13   26  121    1  384  103   18   27   28   48]\n",
            " [   9   14   95   75  164  475   70  228 1137  134  310  218  622]\n",
            " [   0    0    0    0    1    2    1    0    4    2    1    1    1]\n",
            " [   1   11   14   18   25   45   66   35  132   15  241   42  210]\n",
            " [   0    0    0    0    0    5    0    3    4    3    3    2    7]\n",
            " [  20   33  115   98  221  525  250  360 1202  227 1095  332 1777]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00         8\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.00      0.04      0.00        25\n",
            "           5       0.32      0.30      0.31      1805\n",
            "           6       0.03      0.58      0.05        19\n",
            "           7       0.29      0.50      0.37       775\n",
            "           8       0.39      0.32      0.35      3551\n",
            "           9       0.00      0.15      0.01        13\n",
            "          10       0.14      0.28      0.18       855\n",
            "          11       0.00      0.07      0.01        27\n",
            "          12       0.63      0.28      0.39      6255\n",
            "\n",
            "    accuracy                           0.31     13334\n",
            "   macro avg       0.14      0.19      0.13     13334\n",
            "weighted avg       0.47      0.31      0.35     13334\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Model 1: Multinomial Naive Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(x_train_count, y1_train)\n",
        "y_pred = nb.predict(x_val_count)\n",
        "print(accuracy_score(y_pred, y1_test))\n",
        "print(confusion_matrix(y_pred, y1_test))\n",
        "print(classification_report(y_pred, y1_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.31175941202939855\n",
            "[[   0    0    0    0    0    1    0    0    2    1    2    2    1]\n",
            " [   0    1    2    1    2    3    2    3    2    3    5    0    3]\n",
            " [   1    0    2    0    2   10    4    6   27    3   16    4   19]\n",
            " [   0    1    1    4    4   20    1    2   11    1   12    7   20]\n",
            " [   2    2    3   12   35   67    4   24   62   10   22   17   50]\n",
            " [   8    4   24   41  149  535   26  229  305   91  126   96  213]\n",
            " [   0    2    6    7   10   15   84   19   65    5   68   19   99]\n",
            " [   1    1    9   21   61  258   14  606  170   48   93   70  136]\n",
            " [   9   15  109   68  127  389   73  184 1232  126  332  209  636]\n",
            " [   0    0    2    4    9   40    7   21   47   27   26    8   46]\n",
            " [   1   12   28   25   56  108   65   64  241   40  435   66  387]\n",
            " [   2    0    7    3   16   53    9   18   63   17   39   37   64]\n",
            " [  13   24   66   59  102  230  123  130  671  119  586  192 1159]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         9\n",
            "           1       0.02      0.04      0.02        27\n",
            "           2       0.01      0.02      0.01        94\n",
            "           3       0.02      0.05      0.02        84\n",
            "           4       0.06      0.11      0.08       310\n",
            "           5       0.31      0.29      0.30      1847\n",
            "           6       0.20      0.21      0.21       399\n",
            "           7       0.46      0.41      0.43      1488\n",
            "           8       0.43      0.35      0.38      3509\n",
            "           9       0.05      0.11      0.07       237\n",
            "          10       0.25      0.28      0.26      1528\n",
            "          11       0.05      0.11      0.07       328\n",
            "          12       0.41      0.33      0.37      3474\n",
            "\n",
            "    accuracy                           0.31     13334\n",
            "   macro avg       0.17      0.18      0.17     13334\n",
            "weighted avg       0.35      0.31      0.33     13334\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model 2: Linear SVM\n",
        "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
        "lsvm.fit(x_train_count, y1_train)\n",
        "y_pred = lsvm.predict(x_val_count)\n",
        "print(accuracy_score(y_pred, y1_test))\n",
        "print(confusion_matrix(y_pred, y1_test))\n",
        "print(classification_report(y_pred, y1_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3272086395680216\n",
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            " [   0    1    0    1    0    0    1    0    0    0    1    0    0]\n",
            " [   1    0    2    0    1    2    0    0    5    0    3    1    3]\n",
            " [   0    0    0    0    1    4    0    1    5    2    2    2    2]\n",
            " [   0    2    1    6   33   62    3   13   36   11   18    5   31]\n",
            " [   6    4   20   45  134  550   14  282  227   87  104   86  166]\n",
            " [   0    0    1    3    4    4   58    7   25    0   33    9   51]\n",
            " [   1    1    4   14   39  156    6  486  118   28   56   43   77]\n",
            " [  13   20  140  102  211  560  102  276 1619  190  491  294  908]\n",
            " [   0    0    1    3    5   29    3   14   25   24   12    7   29]\n",
            " [   1   14   28   19   40   75   79   62  202   29  430   52  390]\n",
            " [   1    0    2    4   14   43   11   21   41    9   23   26   42]\n",
            " [  14   20   60   48   91  244  135  144  595  111  589  202 1134]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.02      0.25      0.03         4\n",
            "           2       0.01      0.11      0.01        18\n",
            "           3       0.00      0.00      0.00        19\n",
            "           4       0.06      0.15      0.08       221\n",
            "           5       0.32      0.32      0.32      1725\n",
            "           6       0.14      0.30      0.19       195\n",
            "           7       0.37      0.47      0.42      1029\n",
            "           8       0.56      0.33      0.41      4926\n",
            "           9       0.05      0.16      0.07       152\n",
            "          10       0.24      0.30      0.27      1421\n",
            "          11       0.04      0.11      0.05       237\n",
            "          12       0.40      0.33      0.36      3387\n",
            "\n",
            "    accuracy                           0.33     13334\n",
            "   macro avg       0.17      0.22      0.17     13334\n",
            "weighted avg       0.41      0.33      0.35     13334\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Model 3: Logistic Regression\n",
        "logreg = LogisticRegression(C=1)\n",
        "logreg.fit(x_train_count, y1_train)\n",
        "y_pred = logreg.predict(x_val_count)\n",
        "print(accuracy_score(y_pred, y1_test))\n",
        "print(confusion_matrix(y_pred, y1_test))\n",
        "print(classification_report(y_pred, y1_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kesimpulannya, Hasil akurasi yang menggunakan Count vectorizer lebih baik dibandingkan TF-IDF."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
